seed_everything: 42

model:
  class_path: models.slot_autoencoder.SlotAE
  init_args:
    dim: 384 
    learning_rate: 2e-4
    resolution: [224, 224]
    n_slots: [8, 4]
    loss_fn: 
      class_path: torch.nn.MSELoss
    image_encoder_name: 'vit_small_patch14_reg4_dinov2.lvd142m'
    slot_attention_args:
      input_dim: ${..dim}
      slot_dim: ${..dim}
      n_iters: 5
      implicit: True
    sample_strategy:
      - prior
      - prior
    feature_decoder:
      class_path: models.decoder.TransformerDecoder
      init_args:
        dim: ${...dim}
        depth: 4
        resolution: [16, 16]

data:
  class_path: dataset.coco.COCO
  init_args:
    data_dir: ./data/coco/coco2017
    num_workers: 7
    batch_size: 32
    resolution: 224

trainer:
  accumulate_grad_batches: 2
  logger: 
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "semdec"
      save_dir: ./lightning_logs
  val_check_interval: 0.01
  limit_val_batches: 1
  log_every_n_steps: 5
